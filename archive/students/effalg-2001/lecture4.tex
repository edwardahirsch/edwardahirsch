\documentclass[12pt,fleqn,a4paper]{book}
%
% Packages used:
%
\usepackage[koi8-r]{inputenc}
\usepackage[russian, english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
%
% Common customization:
%
\pagestyle{headings}
\newtheorem{theorem}{Теорема}[chapter]
\newtheorem{lemma}{Лемма}[chapter]
\newtheorem{proposition}{Утверждение}[chapter]
\newtheorem{fact}{Факт}[chapter]
\theoremstyle{definition}
\newtheorem{problem}{Задача}[chapter]
\newtheorem{exercise}{Упражнение}[chapter]
\newtheorem{example}{Пример}[chapter]
\newtheorem{definition}{Определение}[chapter]
\newtheorem{remark}{Замечание}[chapter]
\newtheorem{algorithm}{Алгоритм}[chapter]
\def\gap{\medskip\centerline{\fbox{\Huge\bfseries{ПРОБЕЛ В КОНСПЕКТЕ.}}}\medskip}
\advance\headheight by 7pt
\def\headsep{15mm}
\newcommand{\lecture}[3]{%
\def\rightmark{\fbox{\parbox{125mm}{\lecturername: \coursetitle%
\hfil\phantom{.}}}}
\def\leftmark{\fbox{\parbox{125mm}{Лекция {#1}. {#2}%
\hfil\phantom{.}}}}
\renewcommand\chaptername{Лекция}\renewcommand\thechapter{#1}\chapter{{#2}\\{\small (Конспект: {#3})}}
\thispagestyle{headings}}
%
% Things to customize for the course are here:
%
\newcommand\lecturername{Э. А. Гирш}
\newcommand\coursetitle{с/к ``Эффективные алгоритмы''}
%
% Now, this particular lecture definitions:
%
\newcommand{\Rfig}{\mathbb R}
\newcommand{\xopt}{x_{\text{опт}}}
\newcommand{\EE}{\mathbf{E}\,} % expectation
%
% The document
%
\begin{document}
\selectlanguage{russian}
%
% Lecture title
%
\lecture{4}{Приближенные алгоритмы для задач о максимальном сечении
и о мощности объединения множеств}{М. Плискин}
%
% The lecture
%
\section{Приближенные алгоритмы}

Рассмотрим некоторое множество $S$ и функцию $f\colon S\to
\Rfig_+.$ Предположим, что существует элемент $\xopt\in S,$ на
котором функция $f$ достигает своего максимального значения. Наша
задача состоит в том, чтобы найти этот элемент $\xopt.$

Мы будем рассматривать случаи, когда эффективного алгоритма
точного решения нашей задачи не существует или он неизвестен.
Поэтому мы будем искать приближенное решение нашей задачи в
следующем смысле:
\begin{definition}
Алгоритм, с помощью которого можно найти такой элемент $x'\in S,$
что
$$
f(x')\geq\alpha f(\xopt),
$$
называется {\it $\alpha$-приближенным алгоритмом} решения данной
задачи.
\hfill$\qed$
\end{definition}

Данное определение, очевидно, корректно лишь в случае
детерминированных алгоритмов. Для недетерминированных алгоритмов
установим следующее, более общее, определение:
\begin{definition}
Вероятностный алгоритм, в результате работы которого генерируется
элемент $x_{\text{ответ}}\in S$ и при этом
$$
\EE f(x_{\text{ответ}})\geq\alpha f(\xopt),
$$
называется {\it $\alpha$-приближенным вероятностным алгоритмом}
решения данной задачи.
\hfill$\qed$
\end{definition}

\section{Приближенный алгоритм для задачи о максимальном сечении}

\begin{definition}
\label{problem1}
{\bfseries Задача о максимальном сечении (MAX-CUT).}
Пусть есть граф $G=(V,E),$ каждому ребру $e\in E$ которого
приписан некоторый вес $w_e.$ Рассмотрим все возможные раскраски
вершин графа в белый и черный цвета (то есть отображения $j\colon
V\to\{\text{Ч}, \text{Б}\}$\/), и каждой такой раскраске $M$
сопоставим число $f(M),$ вычисляемое как сумма весов ребер, концы
которых имеют в рассматриваемой раскраске $M$ разные цвета
(множество таких ребер назовем {\it сечением}\/). Наша задача
состоит в том, чтобы найти такую раскраску $M_0,$ чтобы $f(M_0)$
было бы максимальным.
\hfill$\qed$
\end{definition}

Будем решать эту задачу приближенно. Сначала переформулируем
задачу как задачу оптимизации:
\begin{definition}\label{problem2}
{\bfseries MAX-CUT как задача целочисленного линейного программирования.}
Сопоставим каждой вершине $v_i\in V$ с номером $i$ число $y_i,$
вычисляемое как
$$
y_i = \begin{cases} -1, &\text{$v_i$ --- белая вершина}\\
1, &\text{$v_i$ -- черная вершина.} \end{cases}
$$
Тогда ребро $\{v_i,v_j\}$ принадлежит сечению в том, и только в
том случае, когда
$$
y_iy_j=-1,
$$
то есть
$$
f(M)=f((y_1,y_2,\ldots,y_n))=\sum_{i<j}\frac{1-y_iy_j}2w_{ij}.
$$
Наша задача состоит в том, чтобы найти максимум $f((y_i)_i)$ по
всем наборам $(y_i)_i$, где каждое $y_i\in\{-1,1\}.$
\hfill$\qed$
\end{definition}

Обозначим решение нашей задачи через $M^*$ и попытаемся сначала
решить более простую задачу:

\begin{definition}
\label{problem3}
{\bfseries Релаксация MAX-CUT: оптимизация на сфере большей размерности.}
Будем искать
$$
\max_{v_1,\ldots,v_n\in S^n} g((v_1,\ldots,v_n)),
$$
где
$$
g((v_1,\ldots,v_n))=\sum_{i>j}\frac{1-v_i\cdot v_j}2w_{ij},
$$
а $S^n$ --- $n$-мерная единичная сфера ($n$ --- число вершин в графе $G$); 
умножение понимается как скалярное произведение.
\hfill$\qed$
\end{definition}

Обозначим этот максимум через $R^*$. Отметим, что
$$
M^*\le R^*.
$$
Этот факт следует из того, что наша новая задача содержит старую
как частный случай (достаточно зафиксировать все переменные, кроме
одной).

Теперь нам необходимо описать способ получения решения старой
задачи из решения новой.

\begin{algorithm}[Randomized rounding]
Пусть у нас есть набор $(v_i)_i$, являющийся решением новой
задачи. Можно считать, что начало у всех векторов помещено в
начало координат. Проведем через начало координат случайным
образом гиперплоскость $T$ размерности $n-1.$ Эта гиперплоскость
разделит пространство на два полупространства $A$ и $B.$ Теперь по
каждому вектору $v_i$ построим число $y_i$ следующим образом:
$$
y_i = \begin{cases} 1,& v_i \in A\\
-1,& v_i\in B.\end{cases}
$$
Иначе говоря, выберем случайным образом вектор $u\in S^n$ 
и построим число $y_i$ так:
$$
y_i = \begin{cases} 1,& u\cdot v_i >0\\
-1,& u\cdot v_i<0\end{cases}
$$
(случай $u\cdot v_i = 0$ будем интерпретировать как попало:
вероятность этого события равна нулю).
\hfill$\qed$
\end{algorithm}

Теперь докажем следующую лемму про только что описанный алгоритм.

\begin{lemma}
Пусть у нас теперь есть набор векторов $v_i$ --- (приближенное) решение
задачи \ref{problem3}, 
$R^\prime$ --- значение $g$ на этом наборе.
%$R^*$ --- соответствующий максимум. 
Пусть
также $(y_i)_i$ - набор, полученный с помощью вышеописанного
алгоритма из набора векторов $v_i$. Обозначим также $U^\prime = f((y_i)_i)$.
Тогда 
$$
\EE U^\prime \ge \alpha R^\prime,
$$
%верна следующая оценка:
%$$
%U^*\geq\alpha(R^*-\varepsilon)\geq(\alpha-\varepsilon)M^*,
%$$
где $\alpha \approx 0.87.$
\end{lemma}
\begin{proof}
Нам надо доказать следующее неравенство:
$$
\EE \sum_{i<j}\frac{1-y_iy_j}2w_{ij}\geq\alpha\sum_{i<j}\frac{1-v_i\cdot
v_j}2w_{ij}.
$$
Возьмем некоторые $y_i$ и $y_j.$ Тогда $y_iy_j=-1$ в том, и только
в том случае, когда вектора $v_i$ и $v_j$ лежат в разных
полупространствах относительно нашей гиперплоскости $T.$ Вычислим
вероятность того, что $y_i\neq y_j$:
$$
P(y_i\neq y_j) = \frac{\theta_{ij}}\pi,
$$
где $\theta_{ij}$ --- угол между векторами $v_i$ и $v_j.$ Отсюда
получаем, используя определение математического ожидания:
$$
\EE \sum_{i<j}\frac{1-y_iy_j}2w_{ij}=\sum\frac{\theta_{ij}}\pi
w_{ij}.
$$
При этом
$$
\sum_{i<j}\frac{1-v_i\cdot
v_j}2w_{ij}=\sum_{i<j}\frac{1-\cos\theta_{ij}}2w_{ij}.
$$
Теперь осталось взять
$$
\alpha = \min_\theta\frac2\pi\frac\theta{1-\cos\theta} \approx
0.87,
$$
и утверждение леммы доказано.
\end{proof}

\begin{definition}
Матрица $Y$ является положительно полуопределенной ($Y\succcurlyeq0$),
если для любого вектора $x$ верно $x^{\mathrm{T}}Yx\ge0$.
\hfill$\qed$
\end{definition}

Теперь еще раз переформулируем нашу задачу.

\begin{definition}
\label{problem4}
{\bfseries MAX-CUT как задача полуопределенного программирования.}
Необходимо максимизировать выражение
$$
\sum\frac{1-y_{ij}}2w_{ij}
$$
по всем матрицам
$$
Y = \{y_{ij}\}_{i,j=1}^n
$$
таким что 
$Y\succcurlyeq0$ и
$\forall i$ $y_{ii}=1$.
\hfill$\qed$
\end{definition}

Приведем следующие три факта без доказательств:
\begin{fact}\label{fact:1}
Задачу \ref{problem4} можно решить приближенно за полиномиальное
время и при этом получить результат $R^*-\varepsilon.$
\end{fact}

\begin{fact}\label{fact:2}
Если матрица $Y$ положительно полуопределена ($Y\succcurlyeq0$),
то она является матрицей скалярных произведений некоторого набора
векторов $v_i$ (заметим, что условие $y_{ii}=1$ гарантирует, что эти вектора ---
единичные), причем эти вектора можно найти за полиномиальное время.
\end{fact}

\begin{theorem}
Мы построили полиномиальный по времени вероятностный
$(\alpha-\varepsilon)$-приближенный алгоритм для $\alpha\approx 0.87$
и произвольного $\varepsilon>0$. 
\end{theorem}
\begin{proof}
Решение нашей задачи, согласно факту~\ref{fact:2}, свелось к решению задачи
\ref{problem4}. Она, по факту~\ref{fact:1}, решается за полиномиальное время
с аддитивной погрешностью не более $\varepsilon'.$ Таким образом, по лемме, мы
построили искомый $(\alpha-\varepsilon)$-приближенный алгоритм. 
\end{proof}

\begin{fact}\label{fact:3}
Этот алгоритм можно сделать детерминированным.
\end{fact}

\begin{fact}\label{fact:4}
Если бы существовал такой алгоритм
для $\alpha>0.95$, то было бы $\mathbf{P}=\mathbf{NP}$.
\end{fact}

Факт~\ref{fact:4} утверждает,
что мы получили довольно хороший результат с теоретической точки зрения.

\section{Метод Монте-Карло}

Рассмотрим следующую проблему, связанную с теорией вероятностей:
пусть мы проводим $m$ испытаний, вероятность успеха в каждом из
которых есть $p_i.$ Определим случайную величину $y_i$ следующим
образом: $y_i$ принимает значение $1$ с вероятностью $p_i$ и
значение $0$ с вероятностью $1-p_i.$ Определим 
$$
\mu = \EE \sum_{i=1}^m y_i = \sum_{i=1}^m p_i.
$$
Теперь, если обозначить
$$
Y=\sum_{i=1}^m y_i,
$$
то справедливы следующие {\it оценки Чернова}:
\begin{fact}
$$
\begin{gathered}
P\{Y>(1+\varepsilon)\mu\}<\left(\frac{e^\varepsilon}{(1+\varepsilon)^{1+\varepsilon}}\right)^\mu
\leq e^{-\frac{\mu\varepsilon^2}4},\\
P\{Y<(1-\varepsilon)\mu\}<e^{-\frac{\mu\varepsilon^2}2}.
\end{gathered}
$$
\end{fact}

Теперь рассмотрим собственно метод Монте-Карло.
\begin{definition}
{\bfseries Задача о нахождении мощности подмножества.}
Пусть у нас есть два конечных множества: $U$ и его подмножество $G$.
При этом мощность множества $U$ известна. Мы хотим найти мощность
множества $G$.
\hfill$\qed$
\end{definition}

Для решения этой задачи существует следующий хорошо известный
вероятностный алгоритм, называемый {\it методом Монте-Карло.\/}

\begin{algorithm}
Будем брать случайным образом точки множества $U$ и проверять их
на принадлежность к множеству $G$. Произведя $m$ экспериментов,
определим величину
$$
\widetilde\rho=\frac{\textstyle\sum_{i=1}^my_i}m
$$
и будем считать, что оня является приближенным значением величины
$$
\rho=\frac{|G|}{|U|}.
$$
Отсюда найдем приближенно мощность множества $G.$
\hfill$\qed$
\end{algorithm}

Очевидно, что чем больше экспериментов мы произведем,
тем точнее мы определим $\rho$.
Из оценок Чернова вытекает следующее соотношение:
$$
P\{\widetilde\rho\notin[(1-\varepsilon)\rho;
(1+\varepsilon)\rho]\}\leq 2e^{-\frac{\rho\varepsilon^2}4},
$$
из которого следует основная теорема:
\begin{theorem}
Для того, чтобы с вероятностью как минимум $1-\delta$ получить
$\widetilde\rho$, удовлетворяющее
$$
(1-\varepsilon)\rho\leq\widetilde\rho\leq(1+\varepsilon)\rho,
$$
достаточно провести
$$
\frac4{\varepsilon^2\rho}\ln\frac2\delta
$$
экспериментов.
\end{theorem}

Таким образом, чтобы знать, сколько экспериментов надо произвести,
надо знать значение $\rho$, но это как раз то, что мы ищем!
Эта проблема преодолима, если мы знаем какую-нибудь
не слишком маленькую нижнюю оценку для $\rho$.

\section{Мощность объединения множеств}
Рассмотрим следующую задачу:
\begin{definition}
{\bfseries Задача о мощности объединения множеств.}
Пусть есть $n$ множеств $H_i$, мощности которых известны.
Необходимо найти мощность их объединения $\bigcup_{i=1}^n H_i$.
\hfill$\qed$
\end{definition}

Для решения этой задачи воспользуемся методом Монте-Карло. Введем
следующие обозначения:
$$
\begin{aligned}
G &= \bigcup\limits_{i=1}^n H_i,\\
U &= \left\{(v,i)\mid v\in H_i\right\},\\
G' &= \left\{(v,i)\in U\,\Big|\, i=\min_{k\in1..n}\{k:v\in
H_k\}\right\}.
\end{aligned}
$$

Отметим, что мощность множества $G'$ совпадает с мощностью нужного
нам объединения:
$$
\left|G'\right|=\left|\bigcup\limits_{i=1}^nH_i\right|.
$$
Определим теперь величину $\rho$ как
$$
\rho=\frac{\left|G'\right|}{|U|}.
$$
Эту величину мы можем узнать из метода Монте-Карло. Мощность
множества $U$ нам также известна:
$$
|U| = \sum_{i=1}^n |H_i|.
$$
Таким образом, мы найдем мощность множества $G',$ то есть решим
нашу задачу.

Осталось лишь оценить количество испытаний. Для этого оценим
величину $\rho$ снизу:
$$
\rho\geq\frac1n.
$$
Эта оценка вытекает из того, что (нестрогое пояснение) <<худший
случай>> достигается, когда все $H_i$ совпадают, а их мощность
равна $N.$ Тогда мощность множества $U$ равна $nN,$ а мощность
объединения равна $N,$ откуда и вытекает требуемое соотношение.

Отсюда получаем выражение для количества испытаний 
$$
m\geq\frac4{\varepsilon^2\rho}\ln\frac2\delta\geq
\frac{4n}{\varepsilon^2}\ln\frac2\delta,
$$
достаточного для того, чтобы вычисленное значение $\rho$
отклонялось от истинного не более чем на $\varepsilon$ с
вероятностью $1-\delta.$

\end{document}
